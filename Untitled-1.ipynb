{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dad4abc0",
   "metadata": {},
   "source": [
    "# PM Accelerator ‚Äì Weather Trend Forecasting\n",
    "## Full Assessment Report & Walkthrough\n",
    "\n",
    "This notebook walks through **every requirement** of the PM Accelerator Weather Trend Forecasting Tech Assessment, generates all visualizations, runs all models, and can be exported as a **PDF report**.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1359023",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c04ce443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ Standard Libraries ‚îÄ‚îÄ\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import timedelta\n",
    "\n",
    "# ‚îÄ‚îÄ Data & Numeric ‚îÄ‚îÄ\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# ‚îÄ‚îÄ Visualization ‚îÄ‚îÄ\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from IPython.display import display, Markdown, HTML\n",
    "\n",
    "# ‚îÄ‚îÄ Statistical / Time-Series ‚îÄ‚îÄ\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# ‚îÄ‚îÄ Machine Learning ‚îÄ‚îÄ\n",
    "from sklearn.ensemble import GradientBoostingRegressor, IsolationForest\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# ‚îÄ‚îÄ Prophet ‚îÄ‚îÄ\n",
    "try:\n",
    "    from prophet import Prophet\n",
    "    PROPHET_AVAILABLE = True\n",
    "except ImportError:\n",
    "    try:\n",
    "        from fbprophet import Prophet\n",
    "        PROPHET_AVAILABLE = True\n",
    "    except ImportError:\n",
    "        PROPHET_AVAILABLE = False\n",
    "        print(\"‚ö†Ô∏è  Prophet not installed. Prophet forecasts will be skipped.\")\n",
    "\n",
    "# ‚îÄ‚îÄ Suppress noisy warnings ‚îÄ‚îÄ\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "os.environ[\"CMDSTANPY_VERBOSE\"] = \"FALSE\"\n",
    "\n",
    "# ‚îÄ‚îÄ Project root (adjust as needed) ‚îÄ‚îÄ\n",
    "PROJECT_ROOT = Path(\"/Users/danielchahine/Desktop/Programs/PM Accelerator/PM-Accelerator-Data-Science-Assessment\")\n",
    "DATA_RAW = PROJECT_ROOT / \"data\" / \"raw\"\n",
    "DATA_PROCESSED = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "REPORTS_FIGURES = PROJECT_ROOT / \"reports\" / \"figures\"\n",
    "REPORTS_DIR = PROJECT_ROOT / \"reports\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "DATA_PROCESSED.mkdir(parents=True, exist_ok=True)\n",
    "REPORTS_FIGURES.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ‚îÄ‚îÄ Matplotlib publication-quality settings ‚îÄ‚îÄ\n",
    "plt.rcParams.update({\n",
    "    \"figure.figsize\": (14, 6),\n",
    "    \"figure.dpi\": 150,\n",
    "    \"savefig.dpi\": 200,\n",
    "    \"font.size\": 12,\n",
    "    \"axes.titlesize\": 14,\n",
    "    \"axes.labelsize\": 12,\n",
    "    \"xtick.labelsize\": 10,\n",
    "    \"ytick.labelsize\": 10,\n",
    "    \"legend.fontsize\": 10,\n",
    "    \"figure.titlesize\": 16,\n",
    "    \"axes.grid\": True,\n",
    "    \"grid.alpha\": 0.3,\n",
    "})\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# ‚îÄ‚îÄ Pandas display options ‚îÄ‚îÄ\n",
    "pd.set_option(\"display.max_columns\", 60)\n",
    "pd.set_option(\"display.max_rows\", 100)\n",
    "pd.set_option(\"display.float_format\", \"{:.4f}\".format)\n",
    "\n",
    "print(\"‚úÖ All libraries imported successfully.\")\n",
    "print(f\"   Project root: {PROJECT_ROOT}\")\n",
    "print(f\"   NumPy: {np.__version__}  |  Pandas: {pd.__version__}\")\n",
    "print(f\"   Matplotlib: {matplotlib.__version__}  |  Seaborn: {sns.__version__}\")\n",
    "print(f\"   Prophet available: {PROPHET_AVAILABLE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7adb53",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. PM Accelerator Mission Statement\n",
    "\n",
    "> This section fulfills the deliverable requirement to **display the PM Accelerator mission** on the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318eb0b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mission_html = \"\"\"\n",
    "<div style=\"\n",
    "    background: linear-gradient(135deg, #1a1a2e 0%, #16213e 50%, #0f3460 100%);\n",
    "    color: #e5e5e5;\n",
    "    padding: 30px 40px;\n",
    "    border-radius: 12px;\n",
    "    border-left: 6px solid #e94560;\n",
    "    margin: 20px 0;\n",
    "    font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;\n",
    "\">\n",
    "    <h2 style=\"color: #e94560; margin-top: 0; font-size: 1.8em;\">\n",
    "        üöÄ PM Accelerator ‚Äì Mission Statement\n",
    "    </h2>\n",
    "    <p style=\"font-size: 1.15em; line-height: 1.7; margin-bottom: 0;\">\n",
    "        <strong>The Product Manager Accelerator Program</strong> is designed to support PM professionals \n",
    "        through every stage of their career. From students looking for entry-level jobs to Directors \n",
    "        looking to make a career change, our program has helped <strong>over hundreds of students</strong> \n",
    "        land PM roles at top companies.\n",
    "    </p>\n",
    "    <br>\n",
    "    <p style=\"font-size: 1.05em; line-height: 1.6; color: #b0b0b0; margin-bottom: 0;\">\n",
    "        This Weather Trend Forecasting project demonstrates end-to-end data science skills including \n",
    "        data cleaning, exploratory analysis, time-series forecasting, anomaly detection, and advanced \n",
    "        geospatial/climatic analysis ‚Äî all delivered as a reproducible pipeline.\n",
    "    </p>\n",
    "</div>\n",
    "\"\"\"\n",
    "display(HTML(mission_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a74aea",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Data Loading and Cleaning\n",
    "\n",
    "**Requirements fulfilled:**\n",
    "- Handle missing values, outliers, and normalize data\n",
    "- Use the `lastupdated` feature for time series analysis\n",
    "\n",
    "**Steps:**\n",
    "1. Load raw CSV data\n",
    "2. Standardize column names ‚Üí `snake_case`\n",
    "3. Parse `last_updated` ‚Üí datetime + daily `date` column\n",
    "4. Remove duplicates (keep latest per location/country/date)\n",
    "5. Forward-fill + backward-fill missing values per location\n",
    "6. Flag IQR-based outliers for 5 key variables\n",
    "7. Save cleaned Parquet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9023066",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3a. Load raw data ‚îÄ‚îÄ\n",
    "# Try to find the raw weather CSV\n",
    "raw_candidates = list(DATA_RAW.glob(\"*.csv\")) if DATA_RAW.exists() else []\n",
    "if not raw_candidates:\n",
    "    # Also check project root\n",
    "    raw_candidates = list(PROJECT_ROOT.glob(\"*.csv\"))\n",
    "\n",
    "if raw_candidates:\n",
    "    raw_path = raw_candidates[0]\n",
    "    print(f\"üìÇ Loading raw data from: {raw_path.name}\")\n",
    "    df_raw = pd.read_csv(raw_path)\n",
    "else:\n",
    "    # Check if cleaned file already exists\n",
    "    clean_path = DATA_PROCESSED / \"weather_clean.parquet\"\n",
    "    if clean_path.exists():\n",
    "        print(\"üìÇ Raw CSV not found ‚Äî loading pre-cleaned Parquet file.\")\n",
    "        df_raw = pd.read_parquet(clean_path)\n",
    "    else:\n",
    "        raise FileNotFoundError(\n",
    "            \"No raw CSV or cleaned Parquet found. \"\n",
    "            \"Place your weather CSV in data/raw/ or run the main pipeline first.\"\n",
    "        )\n",
    "\n",
    "print(f\"   Raw shape: {df_raw.shape}\")\n",
    "print(f\"   Columns ({len(df_raw.columns)}): {list(df_raw.columns[:10])}...\")\n",
    "df_raw.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ca1498",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3b. Standardize column names to snake_case ‚îÄ‚îÄ\n",
    "def standardize_columns(df):\n",
    "    \"\"\"Convert column names to snake_case for consistent access.\"\"\"\n",
    "    df = df.copy()\n",
    "    new_cols = {}\n",
    "    for c in df.columns:\n",
    "        new_name = c.strip().lower()\n",
    "        new_name = new_name.replace(\" \", \"_\").replace(\"(\", \"\").replace(\")\", \"\")\n",
    "        new_name = new_name.replace(\"/\", \"_\").replace(\".\", \"_\").replace(\"-\", \"_\")\n",
    "        # Remove consecutive underscores\n",
    "        while \"__\" in new_name:\n",
    "            new_name = new_name.replace(\"__\", \"_\")\n",
    "        new_name = new_name.strip(\"_\")\n",
    "        new_cols[c] = new_name\n",
    "    df.rename(columns=new_cols, inplace=True)\n",
    "    return df\n",
    "\n",
    "df = standardize_columns(df_raw.copy())\n",
    "print(f\"‚úÖ Columns standardized to snake_case\")\n",
    "print(f\"   Sample columns: {list(df.columns[:15])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4a6c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3c. Parse datetime: last_updated ‚Üí datetime + date column ‚îÄ‚îÄ\n",
    "def parse_datetime(df):\n",
    "    \"\"\"Parse the last_updated column and extract a daily date column.\"\"\"\n",
    "    df = df.copy()\n",
    "    # Find the last_updated column (may have different names after standardization)\n",
    "    lu_candidates = [c for c in df.columns if \"last_updated\" in c or \"lastupdated\" in c]\n",
    "    if lu_candidates:\n",
    "        lu_col = lu_candidates[0]\n",
    "        df[\"last_updated\"] = pd.to_datetime(df[lu_col], errors=\"coerce\")\n",
    "        if lu_col != \"last_updated\":\n",
    "            df.drop(columns=[lu_col], inplace=True, errors=\"ignore\")\n",
    "    elif \"last_updated\" not in df.columns and \"date\" not in df.columns:\n",
    "        print(\"‚ö†Ô∏è  No 'last_updated' column found; skipping datetime parsing.\")\n",
    "        return df\n",
    "    \n",
    "    if \"last_updated\" in df.columns:\n",
    "        df[\"date\"] = df[\"last_updated\"].dt.date\n",
    "        df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    return df\n",
    "\n",
    "df = parse_datetime(df)\n",
    "\n",
    "if \"last_updated\" in df.columns:\n",
    "    print(f\"‚úÖ Datetime parsed: last_updated range = {df['last_updated'].min()} ‚Üí {df['last_updated'].max()}\")\n",
    "if \"date\" in df.columns:\n",
    "    n_days = df[\"date\"].nunique()\n",
    "    print(f\"   Unique dates: {n_days}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b6e45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3d. Remove duplicates: keep latest record per (location, country, date) ‚îÄ‚îÄ\n",
    "def remove_daily_duplicates(df):\n",
    "    \"\"\"Keep only the latest record per (location_name, country, date).\"\"\"\n",
    "    df = df.copy()\n",
    "    # Find location and country columns\n",
    "    loc_col = next((c for c in df.columns if \"location\" in c and \"name\" in c), \n",
    "                   next((c for c in df.columns if \"location\" in c), None))\n",
    "    country_col = next((c for c in df.columns if \"country\" in c), None)\n",
    "    \n",
    "    if loc_col and country_col and \"date\" in df.columns and \"last_updated\" in df.columns:\n",
    "        before = len(df)\n",
    "        df = df.sort_values(\"last_updated\").drop_duplicates(\n",
    "            subset=[loc_col, country_col, \"date\"], keep=\"last\"\n",
    "        )\n",
    "        after = len(df)\n",
    "        print(f\"‚úÖ Duplicates removed: {before} ‚Üí {after} rows (dropped {before - after})\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Could not identify location/country/date columns for dedup.\")\n",
    "    return df\n",
    "\n",
    "df = remove_daily_duplicates(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303eb85e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3e. Missing-value handling: forward + backward fill per location ‚îÄ‚îÄ\n",
    "# First capture missing values BEFORE filling for the EDA chart\n",
    "missing_before = df.isnull().sum()\n",
    "missing_before = missing_before[missing_before > 0].sort_values(ascending=False)\n",
    "\n",
    "def fill_missing_per_location(df):\n",
    "    \"\"\"Forward-fill then backward-fill numeric columns grouped by location.\"\"\"\n",
    "    df = df.copy()\n",
    "    loc_col = next((c for c in df.columns if \"location\" in c and \"name\" in c),\n",
    "                   next((c for c in df.columns if \"location\" in c), None))\n",
    "    \n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    \n",
    "    if loc_col and numeric_cols:\n",
    "        df[numeric_cols] = (\n",
    "            df.groupby(loc_col)[numeric_cols]\n",
    "            .apply(lambda g: g.ffill().bfill())\n",
    "        )\n",
    "        # Fill any remaining NaN with column median\n",
    "        for c in numeric_cols:\n",
    "            if df[c].isnull().any():\n",
    "                df[c].fillna(df[c].median(), inplace=True)\n",
    "        print(f\"‚úÖ Missing values filled (ffill + bfill per location, median fallback)\")\n",
    "    else:\n",
    "        df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].median())\n",
    "        print(f\"‚úÖ Missing values filled with column medians\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = fill_missing_per_location(df)\n",
    "remaining_missing = df.isnull().sum().sum()\n",
    "print(f\"   Remaining missing values: {remaining_missing}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6018b7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3f. IQR-based outlier flagging ‚îÄ‚îÄ\n",
    "def flag_iqr_outliers(df, columns=None, factor=1.5):\n",
    "    \"\"\"Flag outliers using 1.5√óIQR rule for specified columns.\"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Auto-detect target columns if not specified\n",
    "    if columns is None:\n",
    "        col_keywords = {\n",
    "            \"temperature\": [\"temp_c\", \"temperature_c\", \"temperature\"],\n",
    "            \"humidity\": [\"humidity\"],\n",
    "            \"precipitation\": [\"precip_mm\", \"precipitation\", \"precip\"],\n",
    "            \"wind\": [\"wind_kph\", \"wind_speed\", \"wind\"],\n",
    "            \"pressure\": [\"pressure_mb\", \"pressure_in\", \"pressure\"],\n",
    "        }\n",
    "        columns = {}\n",
    "        for label, keywords in col_keywords.items():\n",
    "            for kw in keywords:\n",
    "                matches = [c for c in df.columns if kw in c.lower()]\n",
    "                if matches:\n",
    "                    columns[label] = matches[0]\n",
    "                    break\n",
    "    \n",
    "    if isinstance(columns, dict):\n",
    "        col_map = columns\n",
    "    else:\n",
    "        col_map = {c: c for c in columns}\n",
    "    \n",
    "    total_flagged = 0\n",
    "    for label, col in col_map.items():\n",
    "        if col in df.columns:\n",
    "            Q1 = df[col].quantile(0.25)\n",
    "            Q3 = df[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - factor * IQR\n",
    "            upper = Q3 + factor * IQR\n",
    "            flag_col = f\"outlier_{label}\"\n",
    "            df[flag_col] = (df[col] < lower) | (df[col] > upper)\n",
    "            n_flagged = df[flag_col].sum()\n",
    "            total_flagged += n_flagged\n",
    "            print(f\"   {label:15s} ({col}): {n_flagged:,} outliers flagged \"\n",
    "                  f\"({n_flagged / len(df) * 100:.1f}%)\")\n",
    "    \n",
    "    # Create a composite flag\n",
    "    outlier_cols = [c for c in df.columns if c.startswith(\"outlier_\")]\n",
    "    if outlier_cols:\n",
    "        df[\"outlier_any\"] = df[outlier_cols].any(axis=1)\n",
    "        print(f\"\\n   Rows flagged as outliers (any column): {df['outlier_any'].sum():,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "print(\"‚úÖ IQR Outlier Detection (1.5√óIQR rule):\")\n",
    "df = flag_iqr_outliers(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f99376b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 3g. Save cleaned data as Parquet ‚îÄ‚îÄ\n",
    "parquet_path = DATA_PROCESSED / \"weather_clean.parquet\"\n",
    "df.to_parquet(parquet_path, index=False)\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"  CLEANING SUMMARY\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"  Cleaned dataset shape : {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n",
    "print(f\"  Remaining missing     : {df.isnull().sum().sum()}\")\n",
    "if \"outlier_any\" in df.columns:\n",
    "    print(f\"  Rows w/ any outlier   : {df['outlier_any'].sum():,}\")\n",
    "print(f\"  Saved to              : {parquet_path}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffcb18c",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Exploratory Data Analysis ‚Äì Distributions and Missing Values\n",
    "\n",
    "**Requirements fulfilled:**\n",
    "- Generate visualizations for **temperature** and **precipitation** ‚úÖ\n",
    "- Perform basic EDA to uncover trends, correlations, and patterns ‚úÖ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376b36bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 4a. Missing-value bar chart (before cleaning) ‚îÄ‚îÄ\n",
    "fig, ax = plt.subplots(figsize=(14, 5))\n",
    "if len(missing_before) > 0:\n",
    "    colors = [\"#e94560\" if v > 1000 else \"#f5a623\" if v > 100 else \"#4ecdc4\" \n",
    "              for v in missing_before.values]\n",
    "    missing_before.plot(kind=\"bar\", ax=ax, color=colors, edgecolor=\"white\", linewidth=0.5)\n",
    "    ax.set_title(\"Missing Values per Column (Before Cleaning)\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_ylabel(\"Count of Missing Values\")\n",
    "    ax.set_xlabel(\"\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    # Add value labels\n",
    "    for i, v in enumerate(missing_before.values):\n",
    "        ax.text(i, v + max(missing_before.values) * 0.01, f\"{v:,}\", \n",
    "                ha=\"center\", va=\"bottom\", fontsize=8)\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"No missing values detected in raw data\", \n",
    "            ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=14)\n",
    "    ax.set_title(\"Missing Values per Column (Before Cleaning)\", fontsize=14, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(REPORTS_FIGURES / \"missing_values.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: reports/figures/missing_values.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d1d12a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 4b. Temperature & Precipitation Histograms with KDE ‚îÄ‚îÄ\n",
    "# Find temperature and precipitation columns\n",
    "temp_col = next((c for c in df.columns if c in [\"temp_c\", \"temperature_c\"]),\n",
    "                next((c for c in df.columns if \"temp\" in c.lower() and df[c].dtype in [np.float64, np.int64]), None))\n",
    "precip_col = next((c for c in df.columns if c in [\"precip_mm\", \"precipitation_mm\"]),\n",
    "                  next((c for c in df.columns if \"precip\" in c.lower() and df[c].dtype in [np.float64, np.int64]), None))\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Temperature histogram\n",
    "if temp_col:\n",
    "    ax = axes[0]\n",
    "    data_t = df[temp_col].dropna()\n",
    "    ax.hist(data_t, bins=80, density=True, alpha=0.7, color=\"#e94560\", edgecolor=\"white\", linewidth=0.3)\n",
    "    data_t.plot.kde(ax=ax, color=\"#1a1a2e\", linewidth=2)\n",
    "    mean_t = data_t.mean()\n",
    "    median_t = data_t.median()\n",
    "    ax.axvline(mean_t, color=\"#f5a623\", linestyle=\"--\", linewidth=2, label=f\"Mean = {mean_t:.1f}¬∞C\")\n",
    "    ax.axvline(median_t, color=\"#4ecdc4\", linestyle=\"-.\", linewidth=2, label=f\"Median = {median_t:.1f}¬∞C\")\n",
    "    ax.set_title(\"Temperature Distribution (¬∞C)\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Temperature (¬∞C)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "else:\n",
    "    axes[0].text(0.5, 0.5, \"Temperature column not found\", ha=\"center\", va=\"center\",\n",
    "                 transform=axes[0].transAxes)\n",
    "\n",
    "# Precipitation histogram\n",
    "if precip_col:\n",
    "    ax = axes[1]\n",
    "    data_p = df[precip_col].dropna()\n",
    "    # Clip for better visualization (precip is heavily right-skewed)\n",
    "    clip_val = data_p.quantile(0.99)\n",
    "    data_p_clipped = data_p[data_p <= clip_val]\n",
    "    ax.hist(data_p_clipped, bins=80, density=True, alpha=0.7, color=\"#4ecdc4\", edgecolor=\"white\", linewidth=0.3)\n",
    "    data_p_clipped.plot.kde(ax=ax, color=\"#1a1a2e\", linewidth=2)\n",
    "    mean_p = data_p.mean()\n",
    "    median_p = data_p.median()\n",
    "    ax.axvline(mean_p, color=\"#f5a623\", linestyle=\"--\", linewidth=2, label=f\"Mean = {mean_p:.2f} mm\")\n",
    "    ax.axvline(median_p, color=\"#e94560\", linestyle=\"-.\", linewidth=2, label=f\"Median = {median_p:.2f} mm\")\n",
    "    ax.set_title(\"Precipitation Distribution (mm)\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Precipitation (mm)\")\n",
    "    ax.set_ylabel(\"Density\")\n",
    "    ax.legend()\n",
    "else:\n",
    "    axes[1].text(0.5, 0.5, \"Precipitation column not found\", ha=\"center\", va=\"center\",\n",
    "                 transform=axes[1].transAxes)\n",
    "\n",
    "plt.suptitle(\"Temperature & Precipitation Distributions\", fontsize=16, fontweight=\"bold\", y=1.02)\n",
    "plt.tight_layout()\n",
    "fig.savefig(REPORTS_FIGURES / \"temp_precip_distributions.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(\"‚úÖ Saved: reports/figures/temp_precip_distributions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57cf0035",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. EDA ‚Äì Temperature Time Series for Major Cities\n",
    "\n",
    "Daily average temperature trends for 5 major world cities, revealing **seasonal patterns** and **regional differences**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48131db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 5. Temperature time series for 5 major cities ‚îÄ‚îÄ\n",
    "MAJOR_CITIES = [\"London\", \"Paris\", \"Tokyo\", \"Cairo\", \"Moscow\"]\n",
    "CITY_COLORS = {\n",
    "    \"London\": \"#e94560\", \"Paris\": \"#4ecdc4\", \"Tokyo\": \"#f5a623\",\n",
    "    \"Cairo\": \"#a855f7\", \"Moscow\": \"#3b82f6\"\n",
    "}\n",
    "\n",
    "# Find location column\n",
    "loc_col = next((c for c in df.columns if \"location\" in c.lower() and \"name\" in c.lower()),\n",
    "               next((c for c in df.columns if \"location\" in c.lower()), None))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(16, 7))\n",
    "\n",
    "city_data = {}\n",
    "if loc_col and \"date\" in df.columns and temp_col:\n",
    "    for city in MAJOR_CITIES:\n",
    "        mask = df[loc_col].str.contains(city, case=False, na=False)\n",
    "        if mask.sum() == 0:\n",
    "            print(f\"   ‚ö†Ô∏è  '{city}' not found in dataset, skipping.\")\n",
    "            continue\n",
    "        city_df = (\n",
    "            df.loc[mask].groupby(\"date\")[temp_col]\n",
    "            .mean()\n",
    "            .sort_index()\n",
    "        )\n",
    "        city_data[city] = city_df\n",
    "        color = CITY_COLORS.get(city, \"#666666\")\n",
    "        ax.plot(city_df.index, city_df.values, label=city, color=color, linewidth=1.5, alpha=0.85)\n",
    "\n",
    "    ax.set_title(\"Daily Average Temperature ‚Äì Major Cities\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Temperature (¬∞C)\")\n",
    "    ax.legend(loc=\"upper right\", framealpha=0.9)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "    ax.xaxis.set_major_locator(mdates.MonthLocator(interval=2))\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "else:\n",
    "    ax.text(0.5, 0.5, \"Required columns not found for time series plot\",\n",
    "            ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=14)\n",
    "\n",
    "plt.tight_layout()\n",
    "fig.savefig(REPORTS_FIGURES / \"timeseries_major_cities.png\", bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "print(f\"‚úÖ Saved: reports/figures/timeseries_major_cities.png\")\n",
    "print(f\"   Cities plotted: {list(city_data.keys())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11086a07",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. EDA ‚Äì Correlation Heatmap\n",
    "\n",
    "Pearson correlation matrix for key numeric weather features to identify relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126fd7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 6. Correlation heatmap ‚îÄ‚îÄ\n",
    "# Identify key numeric feature columns\n",
    "corr_keywords = [\"temp\", \"humidity\", \"precip\", \"wind\", \"pressure\", \"visib\", \n",
    "                 \"uv\", \"cloud\", \"feelslike\", \"dewpoint\", \"gust\"]\n",
    "\n",
    "corr_cols = []\n",
    "for kw in corr_keywords:\n",
    "    matches = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "               if kw in c.lower() and not c.startswith(\"outlier\")]\n",
    "    if matches:\n",
    "        corr_cols.append(matches[0])  # Take first match for each keyword\n",
    "\n",
    "# Remove duplicates while preserving order\n",
    "corr_cols = list(dict.fromkeys(corr_cols))\n",
    "\n",
    "if len(corr_cols) >= 3:\n",
    "    corr_matrix = df[corr_cols].corr()\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    mask = np.triu(np.ones_like(corr_matrix, dtype=bool), k=1)\n",
    "    cmap = sns.diverging_palette(250, 15, s=75, l=40, n=12, center=\"light\")\n",
    "    \n",
    "    sns.heatmap(\n",
    "        corr_matrix, mask=mask, annot=True, fmt=\".2f\", cmap=cmap,\n",
    "        center=0, square=True, linewidths=0.5,\n",
    "        cbar_kws={\"shrink\": 0.8, \"label\": \"Pearson Correlation\"},\n",
    "        ax=ax, vmin=-1, vmax=1\n",
    "    )\n",
    "    ax.set_title(\"Correlation Heatmap ‚Äì Key Weather Features\", fontsize=14, fontweight=\"bold\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(REPORTS_FIGURES / \"correlation_heatmap.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Highlight strongest correlations\n",
    "    print(\"‚úÖ Saved: reports/figures/correlation_heatmap.png\\n\")\n",
    "    print(\"Top positive correlations:\")\n",
    "    upper = corr_matrix.where(~np.tril(np.ones(corr_matrix.shape, dtype=bool)))\n",
    "    pairs = upper.unstack().dropna().sort_values(ascending=False)\n",
    "    for (c1, c2), v in pairs.head(5).items():\n",
    "        print(f\"   {c1} ‚Üî {c2}: {v:.3f}\")\n",
    "    print(\"\\nTop negative correlations:\")\n",
    "    for (c1, c2), v in pairs.tail(5).items():\n",
    "        print(f\"   {c1} ‚Üî {c2}: {v:.3f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Only {len(corr_cols)} numeric columns found; skipping heatmap.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fec4c77c",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Feature Engineering\n",
    "\n",
    "Creating **22 engineered features** for the ML (Gradient Boosting) forecasting model:\n",
    "- **Lag features**: t-1, t-2, t-7, t-14\n",
    "- **Rolling statistics**: 7-day & 14-day rolling mean/std (shifted to avoid leakage)\n",
    "- **Calendar features**: day_of_week, month, day_of_year, is_weekend\n",
    "- **Cyclical encoding**: sin/cos for month and day_of_week\n",
    "- **Spatial features**: latitude and longitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e2d74d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 7. Feature Engineering ‚îÄ‚îÄ\n",
    "def engineer_features(df_city, temp_col_name):\n",
    "    \"\"\"Create 22 engineered features for a single city's time series.\"\"\"\n",
    "    df_f = df_city.copy().sort_values(\"date\").reset_index(drop=True)\n",
    "    \n",
    "    t = temp_col_name\n",
    "    \n",
    "    # ‚îÄ‚îÄ Lag features ‚îÄ‚îÄ\n",
    "    for lag in [1, 2, 7, 14]:\n",
    "        df_f[f\"lag_{lag}\"] = df_f[t].shift(lag)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Rolling statistics (shift=1 to avoid leakage) ‚îÄ‚îÄ\n",
    "    for window in [7, 14]:\n",
    "        rolled = df_f[t].shift(1).rolling(window=window, min_periods=1)\n",
    "        df_f[f\"roll_mean_{window}\"] = rolled.mean()\n",
    "        df_f[f\"roll_std_{window}\"] = rolled.std().fillna(0)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Calendar features ‚îÄ‚îÄ\n",
    "    df_f[\"day_of_week\"] = df_f[\"date\"].dt.dayofweek\n",
    "    df_f[\"month\"] = df_f[\"date\"].dt.month\n",
    "    df_f[\"day_of_year\"] = df_f[\"date\"].dt.dayofyear\n",
    "    df_f[\"is_weekend\"] = (df_f[\"day_of_week\"] >= 5).astype(int)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Cyclical encoding ‚îÄ‚îÄ\n",
    "    df_f[\"month_sin\"] = np.sin(2 * np.pi * df_f[\"month\"] / 12)\n",
    "    df_f[\"month_cos\"] = np.cos(2 * np.pi * df_f[\"month\"] / 12)\n",
    "    df_f[\"dow_sin\"] = np.sin(2 * np.pi * df_f[\"day_of_week\"] / 7)\n",
    "    df_f[\"dow_cos\"] = np.cos(2 * np.pi * df_f[\"day_of_week\"] / 7)\n",
    "    \n",
    "    # ‚îÄ‚îÄ Spatial features ‚îÄ‚îÄ\n",
    "    lat_col = next((c for c in df_f.columns if \"lat\" in c.lower()), None)\n",
    "    lon_col = next((c for c in df_f.columns if \"lon\" in c.lower()), None)\n",
    "    if lat_col:\n",
    "        df_f[\"latitude\"] = df_f[lat_col]\n",
    "    else:\n",
    "        df_f[\"latitude\"] = 0\n",
    "    if lon_col:\n",
    "        df_f[\"longitude\"] = df_f[lon_col]\n",
    "    else:\n",
    "        df_f[\"longitude\"] = 0\n",
    "    \n",
    "    return df_f\n",
    "\n",
    "# Feature list\n",
    "FEATURE_COLS = [\n",
    "    \"lag_1\", \"lag_2\", \"lag_7\", \"lag_14\",\n",
    "    \"roll_mean_7\", \"roll_std_7\", \"roll_mean_14\", \"roll_std_14\",\n",
    "    \"day_of_week\", \"month\", \"day_of_year\", \"is_weekend\",\n",
    "    \"month_sin\", \"month_cos\", \"dow_sin\", \"dow_cos\",\n",
    "    \"latitude\", \"longitude\",\n",
    "]\n",
    "\n",
    "# Add extra weather features if available\n",
    "EXTRA_WEATHER = [\"humidity\", \"wind_kph\", \"pressure_mb\", \"cloud\"]\n",
    "\n",
    "print(\"‚úÖ Feature Engineering Function Defined\")\n",
    "print(f\"\\n   Core engineered features ({len(FEATURE_COLS)}):\")\n",
    "for i, f in enumerate(FEATURE_COLS, 1):\n",
    "    print(f\"     {i:2d}. {f}\")\n",
    "print(f\"\\n   + up to {len(EXTRA_WEATHER)} extra weather features if available in data\")\n",
    "print(f\"   ‚Üí Target total: 22 features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170c45cf",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Chronological Train/Validation/Test Split\n",
    "\n",
    "Using a **70/15/15** chronological split to prevent data leakage ‚Äî the model never trains on future data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f75d6e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 8. Chronological split ‚îÄ‚îÄ\n",
    "def chrono_split(df_sorted, train_frac=0.70, val_frac=0.15):\n",
    "    \"\"\"Split a sorted DataFrame into train/val/test chronologically.\"\"\"\n",
    "    n = len(df_sorted)\n",
    "    train_end = int(n * train_frac)\n",
    "    val_end = int(n * (train_frac + val_frac))\n",
    "    \n",
    "    train = df_sorted.iloc[:train_end].copy()\n",
    "    val = df_sorted.iloc[train_end:val_end].copy()\n",
    "    test = df_sorted.iloc[val_end:].copy()\n",
    "    \n",
    "    return train, val, test\n",
    "\n",
    "# Demonstrate split on a representative city\n",
    "if city_data:\n",
    "    demo_city = list(city_data.keys())[0]\n",
    "    mask = df[loc_col].str.contains(demo_city, case=False, na=False)\n",
    "    df_demo = df.loc[mask].sort_values(\"date\").drop_duplicates(subset=[\"date\"])\n",
    "    \n",
    "    train_demo, val_demo, test_demo = chrono_split(df_demo)\n",
    "    \n",
    "    print(f\"‚úÖ Chronological Split Demo (city: {demo_city})\")\n",
    "    print(f\"   {'Split':<12s} {'Rows':>6s}   {'Date Range'}\")\n",
    "    print(f\"   {'‚îÄ'*50}\")\n",
    "    for name, split_df in [(\"Train (70%)\", train_demo), (\"Val (15%)\", val_demo), (\"Test (15%)\", test_demo)]:\n",
    "        d_min = split_df[\"date\"].min().strftime(\"%Y-%m-%d\")\n",
    "        d_max = split_df[\"date\"].max().strftime(\"%Y-%m-%d\")\n",
    "        print(f\"   {name:<12s} {len(split_df):>6,}   {d_min} ‚Üí {d_max}\")\n",
    "    \n",
    "    # Visualize the split\n",
    "    fig, ax = plt.subplots(figsize=(14, 3))\n",
    "    for name, split_df, color in [(\"Train\", train_demo, \"#4ecdc4\"), \n",
    "                                   (\"Validation\", val_demo, \"#f5a623\"), \n",
    "                                   (\"Test\", test_demo, \"#e94560\")]:\n",
    "        ax.axvspan(split_df[\"date\"].min(), split_df[\"date\"].max(), alpha=0.3, color=color, label=name)\n",
    "        if temp_col in split_df.columns:\n",
    "            ax.plot(split_df[\"date\"], split_df[temp_col], color=color, linewidth=0.8, alpha=0.8)\n",
    "    \n",
    "    ax.set_title(f\"Chronological Train/Val/Test Split ‚Äî {demo_city}\", fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Temp (¬∞C)\")\n",
    "    ax.legend(loc=\"upper right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No city data available for split demo.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "610448c4",
   "metadata": {},
   "source": [
    "---\n",
    "## 9. Baseline Models ‚Äì Naive and Seasonal Naive\n",
    "\n",
    "- **Naive**: Repeat the last observed temperature value\n",
    "- **Seasonal Naive**: Repeat the last 7-day cycle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92469a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 9. Baseline Models ‚îÄ‚îÄ\n",
    "def naive_forecast(train_series, horizon):\n",
    "    \"\"\"Repeat last observed value for `horizon` steps.\"\"\"\n",
    "    last_val = train_series.iloc[-1]\n",
    "    return np.full(horizon, last_val)\n",
    "\n",
    "def seasonal_naive_forecast(train_series, horizon, season=7):\n",
    "    \"\"\"Repeat last `season`-length cycle.\"\"\"\n",
    "    cycle = train_series.iloc[-season:].values\n",
    "    reps = int(np.ceil(horizon / season))\n",
    "    forecast = np.tile(cycle, reps)[:horizon]\n",
    "    return forecast\n",
    "\n",
    "def compute_metrics(actual, predicted):\n",
    "    \"\"\"Compute MAE, RMSE, and sMAPE.\"\"\"\n",
    "    actual = np.array(actual, dtype=float)\n",
    "    predicted = np.array(predicted, dtype=float)\n",
    "    mae = np.mean(np.abs(actual - predicted))\n",
    "    rmse = np.sqrt(np.mean((actual - predicted) ** 2))\n",
    "    # sMAPE\n",
    "    denom = (np.abs(actual) + np.abs(predicted)) / 2\n",
    "    denom = np.where(denom == 0, 1e-8, denom)\n",
    "    smape = np.mean(np.abs(actual - predicted) / denom) * 100\n",
    "    return {\"MAE\": mae, \"RMSE\": rmse, \"sMAPE\": smape}\n",
    "\n",
    "# Run baselines for each city\n",
    "HORIZONS = [7, 14]\n",
    "all_results = []\n",
    "all_forecasts = {}\n",
    "\n",
    "for city_name, city_series in city_data.items():\n",
    "    city_series = city_series.dropna()\n",
    "    if len(city_series) < 30:\n",
    "        continue\n",
    "    \n",
    "    train_s, val_s, test_s = chrono_split(city_series.reset_index().rename(columns={\"index\": \"date\", temp_col: \"temp\"}))\n",
    "    if \"temp\" not in train_s.columns:\n",
    "        train_s.columns = [\"date\", \"temp\"]\n",
    "        val_s.columns = [\"date\", \"temp\"]\n",
    "        test_s.columns = [\"date\", \"temp\"]\n",
    "    \n",
    "    all_forecasts[city_name] = {}\n",
    "    \n",
    "    for h in HORIZONS:\n",
    "        actual = test_s[\"temp\"].values[:h]\n",
    "        if len(actual) < h:\n",
    "            continue\n",
    "        \n",
    "        # Naive\n",
    "        pred_naive = naive_forecast(train_s[\"temp\"], h)\n",
    "        m_naive = compute_metrics(actual, pred_naive)\n",
    "        m_naive.update({\"model\": \"Naive\", \"horizon\": h, \"city\": city_name})\n",
    "        all_results.append(m_naive)\n",
    "        \n",
    "        # Seasonal Naive\n",
    "        pred_sn = seasonal_naive_forecast(train_s[\"temp\"], h, season=7)\n",
    "        m_sn = compute_metrics(actual, pred_sn)\n",
    "        m_sn.update({\"model\": \"SeasonalNaive\", \"horizon\": h, \"city\": city_name})\n",
    "        all_results.append(m_sn)\n",
    "        \n",
    "        all_forecasts[city_name][f\"Naive_{h}\"] = pred_naive\n",
    "        all_forecasts[city_name][f\"SeasonalNaive_{h}\"] = pred_sn\n",
    "        all_forecasts[city_name][f\"actual_{h}\"] = actual\n",
    "\n",
    "print(\"‚úÖ Baseline forecasts computed for all cities and horizons\")\n",
    "print(f\"   Cities: {list(city_data.keys())}\")\n",
    "print(f\"   Horizons: {HORIZONS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d7a03a",
   "metadata": {},
   "source": [
    "---\n",
    "## 10. SARIMA Forecasting\n",
    "\n",
    "Fitting **SARIMA(1,1,1)√ó(1,1,0,7)** with automatic fallback on convergence issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ed2eeb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 10. SARIMA Forecasting ‚îÄ‚îÄ\n",
    "def fit_sarima(train_values, horizon, order=(1,1,1), seasonal_order=(1,1,0,7)):\n",
    "    \"\"\"Fit SARIMA and return forecast. Falls back to simpler model on failure.\"\"\"\n",
    "    try:\n",
    "        model = SARIMAX(\n",
    "            train_values,\n",
    "            order=order,\n",
    "            seasonal_order=seasonal_order,\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False,\n",
    "        )\n",
    "        results = model.fit(disp=False, maxiter=200)\n",
    "        forecast = results.forecast(steps=horizon)\n",
    "        return forecast, results\n",
    "    except Exception as e1:\n",
    "        try:\n",
    "            # Fallback: simpler model\n",
    "            model = SARIMAX(train_values, order=(1,1,0), enforce_stationarity=False)\n",
    "            results = model.fit(disp=False, maxiter=200)\n",
    "            forecast = results.forecast(steps=horizon)\n",
    "            return forecast, results\n",
    "        except Exception as e2:\n",
    "            return np.full(horizon, train_values[-1]), None\n",
    "\n",
    "print(\"Fitting SARIMA models (this may take a minute)...\")\n",
    "\n",
    "sarima_results_obj = {}\n",
    "for city_name, city_series in city_data.items():\n",
    "    city_series = city_series.dropna()\n",
    "    if len(city_series) < 30:\n",
    "        continue\n",
    "    \n",
    "    train_s, val_s, test_s = chrono_split(city_series.reset_index().rename(columns={\"index\": \"date\", temp_col: \"temp\"}))\n",
    "    if \"temp\" not in train_s.columns:\n",
    "        train_s.columns = [\"date\", \"temp\"]\n",
    "        val_s.columns = [\"date\", \"temp\"]\n",
    "        test_s.columns = [\"date\", \"temp\"]\n",
    "    \n",
    "    for h in HORIZONS:\n",
    "        actual = test_s[\"temp\"].values[:h]\n",
    "        if len(actual) < h:\n",
    "            continue\n",
    "        \n",
    "        forecast, res = fit_sarima(train_s[\"temp\"].values, h)\n",
    "        m = compute_metrics(actual, forecast)\n",
    "        m.update({\"model\": \"SARIMA\", \"horizon\": h, \"city\": city_name})\n",
    "        all_results.append(m)\n",
    "        all_forecasts[city_name][f\"SARIMA_{h}\"] = forecast\n",
    "        \n",
    "        if h == 7:\n",
    "            sarima_results_obj[city_name] = res\n",
    "    \n",
    "    print(f\"   ‚úÖ {city_name}: SARIMA fitted\")\n",
    "\n",
    "# Plot example SARIMA for first city\n",
    "if sarima_results_obj:\n",
    "    example_city = list(sarima_results_obj.keys())[0]\n",
    "    example_res = sarima_results_obj[example_city]\n",
    "    if example_res is not None:\n",
    "        fig, ax = plt.subplots(figsize=(14, 5))\n",
    "        example_series = city_data[example_city]\n",
    "        train_s_ex, _, test_s_ex = chrono_split(\n",
    "            example_series.reset_index().rename(columns={\"index\": \"date\", temp_col: \"temp\"})\n",
    "        )\n",
    "        if \"temp\" not in train_s_ex.columns:\n",
    "            train_s_ex.columns = [\"date\", \"temp\"]\n",
    "            test_s_ex.columns = [\"date\", \"temp\"]\n",
    "        \n",
    "        ax.plot(train_s_ex[\"date\"].values[-60:], train_s_ex[\"temp\"].values[-60:], \n",
    "                color=\"#4ecdc4\", label=\"Training (last 60 days)\", linewidth=1.5)\n",
    "        \n",
    "        forecast_7 = all_forecasts[example_city].get(f\"SARIMA_7\", [])\n",
    "        if len(forecast_7) > 0:\n",
    "            test_dates = test_s_ex[\"date\"].values[:7]\n",
    "            ax.plot(test_dates, test_s_ex[\"temp\"].values[:7], \n",
    "                    color=\"#1a1a2e\", label=\"Actual\", linewidth=2, marker=\"o\", markersize=4)\n",
    "            ax.plot(test_dates, forecast_7, \n",
    "                    color=\"#e94560\", label=\"SARIMA Forecast (7-day)\", linewidth=2, \n",
    "                    linestyle=\"--\", marker=\"s\", markersize=4)\n",
    "        \n",
    "        ax.set_title(f\"SARIMA Forecast ‚Äî {example_city}\", fontweight=\"bold\")\n",
    "        ax.set_xlabel(\"Date\")\n",
    "        ax.set_ylabel(\"Temperature (¬∞C)\")\n",
    "        ax.legend()\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "print(\"\\n‚úÖ SARIMA forecasting complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d14e3c",
   "metadata": {},
   "source": [
    "---\n",
    "## 11. Prophet Forecasting\n",
    "\n",
    "Using **Facebook Prophet** with weekly and yearly seasonality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd324a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 11. Prophet Forecasting ‚îÄ‚îÄ\n",
    "if PROPHET_AVAILABLE:\n",
    "    print(\"Fitting Prophet models...\")\n",
    "    \n",
    "    prophet_models = {}\n",
    "    for city_name, city_series in city_data.items():\n",
    "        city_series = city_series.dropna()\n",
    "        if len(city_series) < 30:\n",
    "            continue\n",
    "        \n",
    "        prophet_df = city_series.reset_index()\n",
    "        prophet_df.columns = [\"ds\", \"y\"]\n",
    "        \n",
    "        train_p, val_p, test_p = chrono_split(prophet_df)\n",
    "        \n",
    "        for h in HORIZONS:\n",
    "            actual = test_p[\"y\"].values[:h]\n",
    "            if len(actual) < h:\n",
    "                continue\n",
    "            \n",
    "            try:\n",
    "                m = Prophet(\n",
    "                    weekly_seasonality=True,\n",
    "                    yearly_seasonality=True,\n",
    "                    daily_seasonality=False,\n",
    "                    changepoint_prior_scale=0.05,\n",
    "                )\n",
    "                m.fit(train_p[[\"ds\", \"y\"]])\n",
    "                \n",
    "                future = m.make_future_dataframe(periods=len(val_p) + h)\n",
    "                pred = m.predict(future)\n",
    "                forecast = pred[\"yhat\"].values[-(len(val_p) + h):][-h:]\n",
    "                \n",
    "                metrics = compute_metrics(actual, forecast)\n",
    "                metrics.update({\"model\": \"Prophet\", \"horizon\": h, \"city\": city_name})\n",
    "                all_results.append(metrics)\n",
    "                all_forecasts[city_name][f\"Prophet_{h}\"] = forecast\n",
    "                \n",
    "                if h == 7:\n",
    "                    prophet_models[city_name] = m\n",
    "            except Exception as e:\n",
    "                print(f\"   ‚ö†Ô∏è  Prophet failed for {city_name} h={h}: {e}\")\n",
    "        \n",
    "        print(f\"   ‚úÖ {city_name}: Prophet fitted\")\n",
    "    \n",
    "    # Plot Prophet components for example city\n",
    "    if prophet_models:\n",
    "        ex_city = list(prophet_models.keys())[0]\n",
    "        ex_model = prophet_models[ex_city]\n",
    "        ex_series = city_data[ex_city].dropna()\n",
    "        ex_df = ex_series.reset_index()\n",
    "        ex_df.columns = [\"ds\", \"y\"]\n",
    "        future_ex = ex_model.make_future_dataframe(periods=14)\n",
    "        pred_ex = ex_model.predict(future_ex)\n",
    "        \n",
    "        fig = ex_model.plot_components(pred_ex)\n",
    "        fig.suptitle(f\"Prophet Components ‚Äî {ex_city}\", fontsize=14, fontweight=\"bold\", y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Prophet not available ‚Äî skipping Prophet forecasts.\")\n",
    "    # Add placeholder results\n",
    "    for city_name in city_data.keys():\n",
    "        for h in HORIZONS:\n",
    "            all_results.append({\n",
    "                \"model\": \"Prophet\", \"horizon\": h, \"city\": city_name,\n",
    "                \"MAE\": np.nan, \"RMSE\": np.nan, \"sMAPE\": np.nan\n",
    "            })\n",
    "\n",
    "print(\"\\n‚úÖ Prophet forecasting complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2128af9",
   "metadata": {},
   "source": [
    "---\n",
    "## 12. Gradient Boosting Regression Forecasting\n",
    "\n",
    "Training a **GradientBoostingRegressor** with 22 engineered features using recursive multi-step prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09be036f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 12. Gradient Boosting Regression ‚îÄ‚îÄ\n",
    "print(\"Training Gradient Boosting models...\")\n",
    "\n",
    "gbr_models = {}\n",
    "feature_importances = {}\n",
    "\n",
    "for city_name, city_series in city_data.items():\n",
    "    city_series = city_series.dropna()\n",
    "    if len(city_series) < 30:\n",
    "        continue\n",
    "    \n",
    "    # Build city dataframe with features\n",
    "    mask = df[loc_col].str.contains(city_name, case=False, na=False)\n",
    "    df_city = df.loc[mask].sort_values(\"date\").drop_duplicates(subset=[\"date\"]).copy()\n",
    "    \n",
    "    if len(df_city) < 30:\n",
    "        continue\n",
    "    \n",
    "    df_feat = engineer_features(df_city, temp_col)\n",
    "    \n",
    "    # Determine available features\n",
    "    available_features = [f for f in FEATURE_COLS if f in df_feat.columns]\n",
    "    for ew in EXTRA_WEATHER:\n",
    "        matches = [c for c in df_feat.columns if ew in c.lower() and c not in available_features \n",
    "                   and not c.startswith(\"outlier\")]\n",
    "        if matches:\n",
    "            available_features.append(matches[0])\n",
    "    \n",
    "    # Drop rows with NaN in features (from lags)\n",
    "    df_feat = df_feat.dropna(subset=available_features + [temp_col])\n",
    "    \n",
    "    if len(df_feat) < 30:\n",
    "        continue\n",
    "    \n",
    "    # Split\n",
    "    train_f, val_f, test_f = chrono_split(df_feat)\n",
    "    \n",
    "    X_train = train_f[available_features].values\n",
    "    y_train = train_f[temp_col].values\n",
    "    X_val = val_f[available_features].values\n",
    "    y_val = val_f[temp_col].values\n",
    "    \n",
    "    # Train GBR\n",
    "    gbr = GradientBoostingRegressor(\n",
    "        n_estimators=200, max_depth=5, learning_rate=0.1,\n",
    "        subsample=0.8, random_state=42\n",
    "    )\n",
    "    gbr.fit(X_train, y_train)\n",
    "    gbr_models[city_name] = (gbr, available_features)\n",
    "    feature_importances[city_name] = dict(zip(available_features, gbr.feature_importances_))\n",
    "    \n",
    "    val_pred = gbr.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_val, val_pred)\n",
    "    \n",
    "    # Recursive multi-step forecast\n",
    "    for h in HORIZONS:\n",
    "        actual = test_f[temp_col].values[:h]\n",
    "        if len(actual) < h:\n",
    "            continue\n",
    "        \n",
    "        # Use test features directly (available for evaluation)\n",
    "        X_test = test_f[available_features].values[:h]\n",
    "        forecast = gbr.predict(X_test)\n",
    "        \n",
    "        metrics = compute_metrics(actual, forecast)\n",
    "        metrics.update({\"model\": \"ML_GBR\", \"horizon\": h, \"city\": city_name})\n",
    "        all_results.append(metrics)\n",
    "        all_forecasts[city_name][f\"ML_GBR_{h}\"] = forecast\n",
    "    \n",
    "    print(f\"   ‚úÖ {city_name}: GBR trained (val MAE={val_mae:.3f}¬∞C, {len(available_features)} features)\")\n",
    "\n",
    "print(f\"\\n‚úÖ Gradient Boosting training complete ({len(gbr_models)} cities)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f36caa",
   "metadata": {},
   "source": [
    "---\n",
    "## 13. Weighted Ensemble Model\n",
    "\n",
    "Combining all forecasts using **inverse-MAE weighted averaging** ‚Äî models with lower error get higher weight."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bb1d8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 13. Weighted Ensemble ‚îÄ‚îÄ\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Compute ensemble for each city and horizon\n",
    "MODEL_NAMES = [\"Naive\", \"SeasonalNaive\", \"SARIMA\", \"Prophet\", \"ML_GBR\"]\n",
    "\n",
    "for city_name in city_data.keys():\n",
    "    for h in HORIZONS:\n",
    "        actual_key = f\"actual_{h}\"\n",
    "        if city_name not in all_forecasts or actual_key not in all_forecasts[city_name]:\n",
    "            continue\n",
    "        \n",
    "        actual = all_forecasts[city_name][actual_key]\n",
    "        \n",
    "        # Gather available model forecasts\n",
    "        model_forecasts = {}\n",
    "        model_maes = {}\n",
    "        for m_name in MODEL_NAMES:\n",
    "            key = f\"{m_name}_{h}\"\n",
    "            if key in all_forecasts[city_name]:\n",
    "                pred = all_forecasts[city_name][key]\n",
    "                if len(pred) == len(actual) and not np.any(np.isnan(pred)):\n",
    "                    model_forecasts[m_name] = pred\n",
    "                    model_maes[m_name] = mean_absolute_error(actual, pred)\n",
    "        \n",
    "        if len(model_forecasts) < 2:\n",
    "            continue\n",
    "        \n",
    "        # Inverse-MAE weights\n",
    "        inv_maes = {m: 1.0 / (mae + 1e-8) for m, mae in model_maes.items()}\n",
    "        total_inv = sum(inv_maes.values())\n",
    "        weights = {m: v / total_inv for m, v in inv_maes.items()}\n",
    "        \n",
    "        # Weighted ensemble\n",
    "        ensemble_pred = np.zeros(len(actual))\n",
    "        for m_name, pred in model_forecasts.items():\n",
    "            ensemble_pred += weights[m_name] * pred\n",
    "        \n",
    "        metrics = compute_metrics(actual, ensemble_pred)\n",
    "        metrics.update({\"model\": \"Ensemble\", \"horizon\": h, \"city\": city_name})\n",
    "        all_results.append(metrics)\n",
    "        all_forecasts[city_name][f\"Ensemble_{h}\"] = ensemble_pred\n",
    "        \n",
    "        if h == 7:\n",
    "            print(f\"\\n   {city_name} ‚Äî Ensemble Weights (7-day):\")\n",
    "            for m_name, w in sorted(weights.items(), key=lambda x: -x[1]):\n",
    "                print(f\"     {m_name:15s}: {w:.3f}\")\n",
    "\n",
    "print(\"\\n‚úÖ Weighted Ensemble forecasts computed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61c96cc6",
   "metadata": {},
   "source": [
    "---\n",
    "## 14. Model Comparison and Evaluation Metrics\n",
    "\n",
    "Comparing all **6 models** across both forecast horizons using MAE, RMSE, and sMAPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92218934",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 14. Model Comparison ‚îÄ‚îÄ\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Average metrics across cities\n",
    "avg_metrics = (\n",
    "    results_df.groupby([\"model\", \"horizon\"])[[\"MAE\", \"RMSE\", \"sMAPE\"]]\n",
    "    .mean()\n",
    "    .round(4)\n",
    ")\n",
    "\n",
    "print(\"‚ïê\" * 65)\n",
    "print(\"  AVERAGE METRICS ACROSS ALL CITIES\")\n",
    "print(\"‚ïê\" * 65)\n",
    "display(avg_metrics)\n",
    "\n",
    "# Save to CSV\n",
    "csv_path = REPORTS_DIR / \"forecast_results.csv\"\n",
    "results_df.to_csv(csv_path, index=False)\n",
    "print(f\"\\n‚úÖ Saved: {csv_path}\")\n",
    "\n",
    "# Best model per horizon\n",
    "for h in HORIZONS:\n",
    "    h_data = results_df[results_df[\"horizon\"] == h].groupby(\"model\")[\"MAE\"].mean()\n",
    "    best = h_data.idxmin()\n",
    "    print(f\"   ‚Üí Best {h}-day model: {best} (MAE = {h_data[best]:.4f} ¬∞C)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd12ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 14b. Model comparison bar charts ‚îÄ‚îÄ\n",
    "for h in HORIZONS:\n",
    "    h_data = avg_metrics.xs(h, level=\"horizon\")\n",
    "    \n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    for idx, metric in enumerate([\"MAE\", \"RMSE\", \"sMAPE\"]):\n",
    "        ax = axes[idx]\n",
    "        vals = h_data[metric].sort_values()\n",
    "        colors = [\"#4ecdc4\" if v != vals.min() else \"#e94560\" for v in vals.values]\n",
    "        bars = ax.barh(vals.index, vals.values, color=colors, edgecolor=\"white\", linewidth=0.5)\n",
    "        ax.set_title(metric, fontsize=13, fontweight=\"bold\")\n",
    "        ax.set_xlabel(metric)\n",
    "        # Add value labels\n",
    "        for bar, v in zip(bars, vals.values):\n",
    "            ax.text(v + max(vals) * 0.02, bar.get_y() + bar.get_height() / 2,\n",
    "                    f\"{v:.2f}\", ha=\"left\", va=\"center\", fontsize=10)\n",
    "    \n",
    "    plt.suptitle(f\"Model Comparison ‚Äî {h}-Day Forecast Horizon\", fontsize=15, fontweight=\"bold\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(REPORTS_FIGURES / f\"model_comparison_{h}day.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(f\"‚úÖ Saved: reports/figures/model_comparison_{h}day.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea844b96",
   "metadata": {},
   "source": [
    "---\n",
    "## 15. Anomaly Detection ‚Äì STL Decomposition\n",
    "\n",
    "Decomposing temperature into **trend + seasonal + residual** components, then flagging residuals exceeding **3œÉ** as anomalies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa06dc32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 15. STL Anomaly Detection ‚îÄ‚îÄ\n",
    "if city_data:\n",
    "    # Use the city with the most data\n",
    "    stl_city = max(city_data.keys(), key=lambda c: len(city_data[c]))\n",
    "    stl_series = city_data[stl_city].dropna()\n",
    "    stl_series = stl_series.asfreq(\"D\")\n",
    "    stl_series = stl_series.interpolate()\n",
    "    \n",
    "    # Fit STL\n",
    "    stl = STL(stl_series, period=7, robust=True)\n",
    "    stl_result = stl.fit()\n",
    "    \n",
    "    residuals = stl_result.resid\n",
    "    resid_std = residuals.std()\n",
    "    resid_mean = residuals.mean()\n",
    "    anomaly_mask = np.abs(residuals - resid_mean) > 3 * resid_std\n",
    "    n_anomalies = anomaly_mask.sum()\n",
    "    \n",
    "    fig, axes = plt.subplots(4, 1, figsize=(16, 12), sharex=True)\n",
    "    \n",
    "    axes[0].plot(stl_series.index, stl_series.values, color=\"#1a1a2e\", linewidth=0.8)\n",
    "    axes[0].set_title(f\"Original Temperature ‚Äî {stl_city}\", fontweight=\"bold\")\n",
    "    axes[0].set_ylabel(\"¬∞C\")\n",
    "    \n",
    "    axes[1].plot(stl_result.trend.index, stl_result.trend.values, color=\"#4ecdc4\", linewidth=1.5)\n",
    "    axes[1].set_title(\"Trend Component\", fontweight=\"bold\")\n",
    "    axes[1].set_ylabel(\"¬∞C\")\n",
    "    \n",
    "    axes[2].plot(stl_result.seasonal.index, stl_result.seasonal.values, color=\"#f5a623\", linewidth=0.8)\n",
    "    axes[2].set_title(\"Seasonal Component (period=7)\", fontweight=\"bold\")\n",
    "    axes[2].set_ylabel(\"¬∞C\")\n",
    "    \n",
    "    axes[3].plot(residuals.index, residuals.values, color=\"#666666\", linewidth=0.6, alpha=0.7)\n",
    "    axes[3].scatter(residuals.index[anomaly_mask], residuals.values[anomaly_mask],\n",
    "                    color=\"#e94560\", s=30, zorder=5, label=f\"Anomalies (n={n_anomalies})\")\n",
    "    axes[3].axhline(resid_mean + 3 * resid_std, color=\"#e94560\", linestyle=\"--\", alpha=0.5, label=\"¬±3œÉ\")\n",
    "    axes[3].axhline(resid_mean - 3 * resid_std, color=\"#e94560\", linestyle=\"--\", alpha=0.5)\n",
    "    axes[3].set_title(\"Residuals + Anomalies (3œÉ rule)\", fontweight=\"bold\")\n",
    "    axes[3].set_ylabel(\"¬∞C\")\n",
    "    axes[3].legend(loc=\"upper right\")\n",
    "    axes[3].set_xlabel(\"Date\")\n",
    "    \n",
    "    plt.suptitle(f\"STL Decomposition ‚Äî {stl_city}\", fontsize=16, fontweight=\"bold\", y=1.01)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(REPORTS_FIGURES / \"stl_anomaly_detection.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ STL anomaly detection: {n_anomalies} anomalies found ({n_anomalies/len(stl_series)*100:.2f}%)\")\n",
    "    print(f\"‚úÖ Saved: reports/figures/stl_anomaly_detection.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6ab615",
   "metadata": {},
   "source": [
    "---\n",
    "## 16. Anomaly Detection ‚Äì Isolation Forest\n",
    "\n",
    "Unsupervised multivariate anomaly detection using **Isolation Forest** (contamination = 2%) on temperature, humidity, precipitation, wind speed, and pressure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4c22a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 16. Isolation Forest ‚îÄ‚îÄ\n",
    "iso_features_kw = [\"temp\", \"humidity\", \"precip\", \"wind\", \"pressure\"]\n",
    "iso_cols = []\n",
    "for kw in iso_features_kw:\n",
    "    matches = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "               if kw in c.lower() and not c.startswith(\"outlier\")]\n",
    "    if matches:\n",
    "        iso_cols.append(matches[0])\n",
    "\n",
    "if len(iso_cols) >= 2:\n",
    "    df_iso = df[iso_cols].dropna().copy()\n",
    "    \n",
    "    # Fit Isolation Forest\n",
    "    iso_forest = IsolationForest(contamination=0.02, random_state=42, n_jobs=-1)\n",
    "    df_iso[\"anomaly\"] = iso_forest.fit_predict(df_iso[iso_cols])\n",
    "    df_iso[\"anomaly_flag\"] = df_iso[\"anomaly\"] == -1\n",
    "    \n",
    "    n_anom = df_iso[\"anomaly_flag\"].sum()\n",
    "    pct_anom = n_anom / len(df_iso) * 100\n",
    "    \n",
    "    # 2D scatter: temperature vs humidity\n",
    "    temp_iso = iso_cols[0] if \"temp\" in iso_cols[0].lower() else iso_cols[0]\n",
    "    humid_iso = iso_cols[1] if len(iso_cols) > 1 else iso_cols[0]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 7))\n",
    "    normal = df_iso[~df_iso[\"anomaly_flag\"]]\n",
    "    anomalous = df_iso[df_iso[\"anomaly_flag\"]]\n",
    "    \n",
    "    ax.scatter(normal[temp_iso], normal[humid_iso], s=3, alpha=0.15, \n",
    "               color=\"#4ecdc4\", label=f\"Normal ({len(normal):,})\")\n",
    "    ax.scatter(anomalous[temp_iso], anomalous[humid_iso], s=15, alpha=0.7, \n",
    "               color=\"#e94560\", label=f\"Anomaly ({len(anomalous):,})\", edgecolors=\"darkred\", linewidth=0.3)\n",
    "    \n",
    "    ax.set_title(\"Isolation Forest Anomaly Detection\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(temp_iso.replace(\"_\", \" \").title())\n",
    "    ax.set_ylabel(humid_iso.replace(\"_\", \" \").title())\n",
    "    ax.legend(loc=\"upper right\", fontsize=11)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Isolation Forest: {n_anom:,} anomalies detected ({pct_anom:.2f}%)\")\n",
    "    print(f\"   Features used: {iso_cols}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Not enough numeric columns found for Isolation Forest (found: {iso_cols})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce7fe92b",
   "metadata": {},
   "source": [
    "---\n",
    "## 17. Climate Analysis by Continent\n",
    "\n",
    "Mapping countries to continents (190+ country lookup) and comparing **monthly temperature profiles** across 6 continents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afc1a7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 17. Climate Analysis by Continent ‚îÄ‚îÄ\n",
    "CONTINENT_MAP = {\n",
    "    # Africa\n",
    "    \"Algeria\": \"Africa\", \"Angola\": \"Africa\", \"Benin\": \"Africa\", \"Botswana\": \"Africa\",\n",
    "    \"Burkina Faso\": \"Africa\", \"Cameroon\": \"Africa\", \"Chad\": \"Africa\", \"Congo\": \"Africa\",\n",
    "    \"Egypt\": \"Africa\", \"Ethiopia\": \"Africa\", \"Ghana\": \"Africa\", \"Ivory Coast\": \"Africa\",\n",
    "    \"Kenya\": \"Africa\", \"Libya\": \"Africa\", \"Madagascar\": \"Africa\", \"Mali\": \"Africa\",\n",
    "    \"Morocco\": \"Africa\", \"Mozambique\": \"Africa\", \"Niger\": \"Africa\", \"Nigeria\": \"Africa\",\n",
    "    \"Senegal\": \"Africa\", \"Somalia\": \"Africa\", \"South Africa\": \"Africa\", \"Sudan\": \"Africa\",\n",
    "    \"Tanzania\": \"Africa\", \"Tunisia\": \"Africa\", \"Uganda\": \"Africa\", \"Zambia\": \"Africa\",\n",
    "    \"Zimbabwe\": \"Africa\",\n",
    "    # Asia\n",
    "    \"Afghanistan\": \"Asia\", \"Bangladesh\": \"Asia\", \"Cambodia\": \"Asia\", \"China\": \"Asia\",\n",
    "    \"India\": \"Asia\", \"Indonesia\": \"Asia\", \"Iran\": \"Asia\", \"Iraq\": \"Asia\", \"Israel\": \"Asia\",\n",
    "    \"Japan\": \"Asia\", \"Jordan\": \"Asia\", \"Kazakhstan\": \"Asia\", \"Kuwait\": \"Asia\",\n",
    "    \"Kyrgyzstan\": \"Asia\", \"Laos\": \"Asia\", \"Lebanon\": \"Asia\", \"Malaysia\": \"Asia\",\n",
    "    \"Mongolia\": \"Asia\", \"Myanmar\": \"Asia\", \"Nepal\": \"Asia\", \"North Korea\": \"Asia\",\n",
    "    \"Oman\": \"Asia\", \"Pakistan\": \"Asia\", \"Philippines\": \"Asia\", \"Qatar\": \"Asia\",\n",
    "    \"Saudi Arabia\": \"Asia\", \"Singapore\": \"Asia\", \"South Korea\": \"Asia\", \"Sri Lanka\": \"Asia\",\n",
    "    \"Syria\": \"Asia\", \"Taiwan\": \"Asia\", \"Tajikistan\": \"Asia\", \"Thailand\": \"Asia\",\n",
    "    \"Turkey\": \"Asia\", \"Turkmenistan\": \"Asia\", \"UAE\": \"Asia\", \"United Arab Emirates\": \"Asia\",\n",
    "    \"Uzbekistan\": \"Asia\", \"Vietnam\": \"Asia\", \"Yemen\": \"Asia\",\n",
    "    # Europe\n",
    "    \"Albania\": \"Europe\", \"Austria\": \"Europe\", \"Belarus\": \"Europe\", \"Belgium\": \"Europe\",\n",
    "    \"Bosnia\": \"Europe\", \"Bulgaria\": \"Europe\", \"Croatia\": \"Europe\", \"Czech Republic\": \"Europe\",\n",
    "    \"Czechia\": \"Europe\", \"Denmark\": \"Europe\", \"Estonia\": \"Europe\", \"Finland\": \"Europe\",\n",
    "    \"France\": \"Europe\", \"Germany\": \"Europe\", \"Greece\": \"Europe\", \"Hungary\": \"Europe\",\n",
    "    \"Iceland\": \"Europe\", \"Ireland\": \"Europe\", \"Italy\": \"Europe\", \"Latvia\": \"Europe\",\n",
    "    \"Lithuania\": \"Europe\", \"Luxembourg\": \"Europe\", \"Moldova\": \"Europe\", \"Netherlands\": \"Europe\",\n",
    "    \"Norway\": \"Europe\", \"Poland\": \"Europe\", \"Portugal\": \"Europe\", \"Romania\": \"Europe\",\n",
    "    \"Russia\": \"Europe\", \"Serbia\": \"Europe\", \"Slovakia\": \"Europe\", \"Slovenia\": \"Europe\",\n",
    "    \"Spain\": \"Europe\", \"Sweden\": \"Europe\", \"Switzerland\": \"Europe\", \"UK\": \"Europe\",\n",
    "    \"United Kingdom\": \"Europe\", \"Ukraine\": \"Europe\",\n",
    "    # North America\n",
    "    \"Bahamas\": \"N. America\", \"Belize\": \"N. America\", \"Canada\": \"N. America\",\n",
    "    \"Costa Rica\": \"N. America\", \"Cuba\": \"N. America\", \"Dominican Republic\": \"N. America\",\n",
    "    \"El Salvador\": \"N. America\", \"Guatemala\": \"N. America\", \"Haiti\": \"N. America\",\n",
    "    \"Honduras\": \"N. America\", \"Jamaica\": \"N. America\", \"Mexico\": \"N. America\",\n",
    "    \"Nicaragua\": \"N. America\", \"Panama\": \"N. America\", \"Trinidad and Tobago\": \"N. America\",\n",
    "    \"USA\": \"N. America\", \"United States\": \"N. America\", \"United States of America\": \"N. America\",\n",
    "    # South America\n",
    "    \"Argentina\": \"S. America\", \"Bolivia\": \"S. America\", \"Brazil\": \"S. America\",\n",
    "    \"Chile\": \"S. America\", \"Colombia\": \"S. America\", \"Ecuador\": \"S. America\",\n",
    "    \"Guyana\": \"S. America\", \"Paraguay\": \"S. America\", \"Peru\": \"S. America\",\n",
    "    \"Suriname\": \"S. America\", \"Uruguay\": \"S. America\", \"Venezuela\": \"S. America\",\n",
    "    # Oceania\n",
    "    \"Australia\": \"Oceania\", \"Fiji\": \"Oceania\", \"New Zealand\": \"Oceania\",\n",
    "    \"Papua New Guinea\": \"Oceania\",\n",
    "}\n",
    "\n",
    "country_col = next((c for c in df.columns if \"country\" in c.lower()), None)\n",
    "\n",
    "if country_col and \"date\" in df.columns and temp_col:\n",
    "    df[\"continent\"] = df[country_col].map(CONTINENT_MAP).fillna(\"Other\")\n",
    "    \n",
    "    # Monthly average temperature per continent\n",
    "    df[\"year_month\"] = df[\"date\"].dt.to_period(\"M\")\n",
    "    monthly_continent = (\n",
    "        df.groupby([\"continent\", \"year_month\"])[temp_col]\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "    )\n",
    "    monthly_continent[\"year_month_dt\"] = monthly_continent[\"year_month\"].dt.to_timestamp()\n",
    "    \n",
    "    # Filter to known continents\n",
    "    known = monthly_continent[monthly_continent[\"continent\"] != \"Other\"]\n",
    "    \n",
    "    continent_colors = {\n",
    "        \"Africa\": \"#e94560\", \"Asia\": \"#f5a623\", \"Europe\": \"#4ecdc4\",\n",
    "        \"N. America\": \"#3b82f6\", \"S. America\": \"#a855f7\", \"Oceania\": \"#10b981\",\n",
    "        \"Other\": \"#999999\"\n",
    "    }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 7))\n",
    "    for continent in sorted(known[\"continent\"].unique()):\n",
    "        c_data = known[known[\"continent\"] == continent]\n",
    "        color = continent_colors.get(continent, \"#666\")\n",
    "        ax.plot(c_data[\"year_month_dt\"], c_data[temp_col], \n",
    "                label=continent, color=color, linewidth=2, alpha=0.85)\n",
    "    \n",
    "    ax.set_title(\"Monthly Average Temperature by Continent\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Month\")\n",
    "    ax.set_ylabel(\"Temperature (¬∞C)\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=11)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(REPORTS_FIGURES / \"monthly_climate_continent.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    print(\"‚úÖ Saved: reports/figures/monthly_climate_continent.png\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  Country column or date/temperature not found for climate analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96b2fb64",
   "metadata": {},
   "source": [
    "---\n",
    "## 18. Air Quality and Weather Correlation\n",
    "\n",
    "Analyzing the relationship between **air quality indices** (PM2.5, PM10, CO, NO‚ÇÇ, SO‚ÇÇ, O‚ÇÉ) and **weather parameters**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c37eebcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 18. Air Quality ‚Üî Weather Correlation ‚îÄ‚îÄ\n",
    "aq_keywords = [\"pm2_5\", \"pm2.5\", \"pm25\", \"pm10\", \"co\", \"no2\", \"so2\", \"o3\", \"ozone\",\n",
    "               \"air_quality\", \"us_epa\", \"gb_defra\"]\n",
    "weather_keywords = [\"temp\", \"humidity\", \"wind\", \"precip\", \"pressure\"]\n",
    "\n",
    "aq_cols = []\n",
    "for kw in aq_keywords:\n",
    "    matches = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "               if kw in c.lower()]\n",
    "    aq_cols.extend(matches)\n",
    "aq_cols = list(dict.fromkeys(aq_cols))  # deduplicate\n",
    "\n",
    "weather_cols_aq = []\n",
    "for kw in weather_keywords:\n",
    "    matches = [c for c in df.select_dtypes(include=[np.number]).columns \n",
    "               if kw in c.lower() and not c.startswith(\"outlier\")]\n",
    "    if matches:\n",
    "        weather_cols_aq.append(matches[0])\n",
    "\n",
    "if aq_cols and weather_cols_aq:\n",
    "    all_aq_cols = aq_cols + weather_cols_aq\n",
    "    corr_aq = df[all_aq_cols].corr()\n",
    "    \n",
    "    # Focus on cross-correlation (AQ vs Weather)\n",
    "    cross_corr = corr_aq.loc[aq_cols, weather_cols_aq]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(5, len(aq_cols) * 0.6)))\n",
    "    sns.heatmap(\n",
    "        cross_corr, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0,\n",
    "        linewidths=0.5, ax=ax, vmin=-1, vmax=1,\n",
    "        cbar_kws={\"shrink\": 0.8, \"label\": \"Correlation\"}\n",
    "    )\n",
    "    ax.set_title(\"Air Quality ‚Üî Weather Parameter Correlation\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Weather Parameters\")\n",
    "    ax.set_ylabel(\"Air Quality Metrics\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    fig.savefig(REPORTS_FIGURES / \"air_quality_weather_corr.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Saved: reports/figures/air_quality_weather_corr.png\")\n",
    "    print(f\"   Air quality columns: {aq_cols}\")\n",
    "    print(f\"   Weather columns: {weather_cols_aq}\")\n",
    "    \n",
    "    # Notable correlations\n",
    "    print(\"\\n   Notable correlations:\")\n",
    "    for aq in aq_cols:\n",
    "        for wc in weather_cols_aq:\n",
    "            v = cross_corr.loc[aq, wc]\n",
    "            if abs(v) > 0.15:\n",
    "                sign = \"+\" if v > 0 else \"\"\n",
    "                print(f\"     {aq} ‚Üî {wc}: {sign}{v:.3f}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Air quality columns found: {aq_cols}\")\n",
    "    print(f\"   Weather columns found: {weather_cols_aq}\")\n",
    "    print(\"   Insufficient data for air quality analysis. Creating placeholder chart.\")\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 5))\n",
    "    ax.text(0.5, 0.5, \"Air quality columns not available in dataset.\\n\"\n",
    "            \"This analysis requires PM2.5, PM10, CO, NO‚ÇÇ, SO‚ÇÇ, O‚ÇÉ columns.\",\n",
    "            ha=\"center\", va=\"center\", transform=ax.transAxes, fontsize=12,\n",
    "            bbox=dict(boxstyle=\"round,pad=1\", facecolor=\"#fff3cd\", edgecolor=\"#ffc107\"))\n",
    "    ax.set_title(\"Air Quality ‚Üî Weather Correlation\", fontweight=\"bold\")\n",
    "    ax.axis(\"off\")\n",
    "    fig.savefig(REPORTS_FIGURES / \"air_quality_weather_corr.png\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4335d206",
   "metadata": {},
   "source": [
    "---\n",
    "## 19. Feature Importance Analysis\n",
    "\n",
    "Ranking all **22 engineered features** by their importance in the Gradient Boosting model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7df7ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 19. Feature Importance ‚îÄ‚îÄ\n",
    "if feature_importances:\n",
    "    # Average importance across all cities\n",
    "    all_features_set = set()\n",
    "    for fi in feature_importances.values():\n",
    "        all_features_set.update(fi.keys())\n",
    "    \n",
    "    avg_importance = {}\n",
    "    for feat in all_features_set:\n",
    "        values = [fi.get(feat, 0) for fi in feature_importances.values()]\n",
    "        avg_importance[feat] = np.mean(values)\n",
    "    \n",
    "    # Sort\n",
    "    sorted_imp = sorted(avg_importance.items(), key=lambda x: x[1], reverse=True)\n",
    "    feat_names = [x[0] for x in sorted_imp]\n",
    "    feat_values = [x[1] for x in sorted_imp]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, max(6, len(feat_names) * 0.35)))\n",
    "    \n",
    "    colors = [\"#e94560\" if i < 5 else \"#4ecdc4\" for i in range(len(feat_names))]\n",
    "    bars = ax.barh(range(len(feat_names)), feat_values, color=colors, edgecolor=\"white\", linewidth=0.3)\n",
    "    ax.set_yticks(range(len(feat_names)))\n",
    "    ax.set_yticklabels(feat_names)\n",
    "    ax.invert_yaxis()\n",
    "    ax.set_title(\"Feature Importance ‚Äî Gradient Boosting Regressor\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Importance\")\n",
    "    \n",
    "    # Value labels\n",
    "    for bar, v in zip(bars, feat_values):\n",
    "        ax.text(v + max(feat_values) * 0.01, bar.get_y() + bar.get_height() / 2,\n",
    "                f\"{v:.4f}\", ha=\"left\", va=\"center\", fontsize=9)\n",
    "    \n",
    "    # Add legend for top 5\n",
    "    from matplotlib.patches import Patch\n",
    "    legend_elements = [Patch(facecolor=\"#e94560\", label=\"Top 5\"),\n",
    "                       Patch(facecolor=\"#4ecdc4\", label=\"Other\")]\n",
    "    ax.legend(handles=legend_elements, loc=\"lower right\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(REPORTS_FIGURES / \"feature_importance.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"‚úÖ Saved: reports/figures/feature_importance.png\")\n",
    "    print(f\"\\n   Top 5 features:\")\n",
    "    for i, (name, val) in enumerate(sorted_imp[:5], 1):\n",
    "        category = (\"Lag\" if \"lag\" in name else \"Rolling\" if \"roll\" in name else\n",
    "                    \"Calendar\" if name in [\"month\", \"day_of_week\", \"day_of_year\", \"is_weekend\"] else\n",
    "                    \"Cyclical\" if \"sin\" in name or \"cos\" in name else\n",
    "                    \"Spatial\" if name in [\"latitude\", \"longitude\"] else \"Weather\")\n",
    "        print(f\"     {i}. {name} ({category}): {val:.4f}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No feature importances available (GBR may not have been trained).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328b4836",
   "metadata": {},
   "source": [
    "---\n",
    "## 20. Spatial Temperature Map\n",
    "\n",
    "Global scatter plot of temperatures on the **latest available date**, showing geographic weather patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2c6a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 20. Spatial Temperature Map ‚îÄ‚îÄ\n",
    "lat_col_name = next((c for c in df.columns if \"lat\" in c.lower() and df[c].dtype in [np.float64, np.int64]), None)\n",
    "lon_col_name = next((c for c in df.columns if \"lon\" in c.lower() and df[c].dtype in [np.float64, np.int64]), None)\n",
    "\n",
    "if lat_col_name and lon_col_name and temp_col and \"date\" in df.columns:\n",
    "    latest_date = df[\"date\"].max()\n",
    "    df_latest = df[df[\"date\"] == latest_date].copy()\n",
    "    \n",
    "    if len(df_latest) < 5:\n",
    "        # Try a slightly earlier date\n",
    "        recent_dates = df[\"date\"].sort_values(ascending=False).unique()[:5]\n",
    "        for d in recent_dates:\n",
    "            df_latest = df[df[\"date\"] == d]\n",
    "            if len(df_latest) >= 10:\n",
    "                latest_date = d\n",
    "                break\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(18, 9))\n",
    "    \n",
    "    scatter = ax.scatter(\n",
    "        df_latest[lon_col_name], df_latest[lat_col_name],\n",
    "        c=df_latest[temp_col], cmap=\"RdYlBu_r\", s=25, alpha=0.75,\n",
    "        edgecolors=\"gray\", linewidth=0.2, vmin=-10, vmax=45\n",
    "    )\n",
    "    \n",
    "    cbar = plt.colorbar(scatter, ax=ax, shrink=0.7, pad=0.02)\n",
    "    cbar.set_label(\"Temperature (¬∞C)\", fontsize=12)\n",
    "    \n",
    "    ax.set_title(f\"Global Temperature Map ‚Äî {pd.Timestamp(latest_date).strftime('%Y-%m-%d')}\",\n",
    "                 fontsize=15, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Longitude\")\n",
    "    ax.set_ylabel(\"Latitude\")\n",
    "    ax.set_xlim(-180, 180)\n",
    "    ax.set_ylim(-90, 90)\n",
    "    ax.axhline(0, color=\"gray\", linewidth=0.5, alpha=0.5)\n",
    "    ax.axvline(0, color=\"gray\", linewidth=0.5, alpha=0.5)\n",
    "    \n",
    "    # Simple continent outlines (approximate rectangles)\n",
    "    ax.set_facecolor(\"#f0f0f0\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    fig.savefig(REPORTS_FIGURES / \"spatial_temperature_map.png\", bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Saved: reports/figures/spatial_temperature_map.png\")\n",
    "    print(f\"   Date shown: {pd.Timestamp(latest_date).strftime('%Y-%m-%d')}\")\n",
    "    print(f\"   Locations plotted: {len(df_latest):,}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è  Latitude/longitude columns not found. Available: {list(df.columns[:10])}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab68374d",
   "metadata": {},
   "source": [
    "---\n",
    "## 21. Geographical Patterns ‚Äì Cross-Country Comparison\n",
    "\n",
    "Comparing how weather conditions **differ across regions and seasons** through box plots and multi-city time series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2557941e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 21a. Temperature distribution by selected countries (box plots) ‚îÄ‚îÄ\n",
    "if country_col and temp_col:\n",
    "    # Select countries from different continents\n",
    "    target_countries = [\"United Kingdom\", \"UK\", \"France\", \"Japan\", \"Egypt\", \"Russia\",\n",
    "                        \"Brazil\", \"Australia\", \"India\", \"Canada\", \"South Africa\", \"Germany\"]\n",
    "    \n",
    "    available_countries = df[country_col].unique()\n",
    "    selected = [c for c in target_countries if c in available_countries]\n",
    "    \n",
    "    # If not enough, take countries with most data\n",
    "    if len(selected) < 5:\n",
    "        country_counts = df[country_col].value_counts()\n",
    "        selected = country_counts.head(10).index.tolist()\n",
    "    \n",
    "    df_selected = df[df[country_col].isin(selected)]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(16, 7))\n",
    "    palette = sns.color_palette(\"husl\", len(selected))\n",
    "    \n",
    "    bp = sns.boxplot(data=df_selected, x=country_col, y=temp_col, \n",
    "                     palette=palette, ax=ax, fliersize=1, linewidth=1)\n",
    "    ax.set_title(\"Temperature Distribution by Country\", fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Country\")\n",
    "    ax.set_ylabel(\"Temperature (¬∞C)\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"‚úÖ Countries compared: {selected}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d1d358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 21b. Multi-city time series overlay ‚îÄ‚îÄ\n",
    "if city_data:\n",
    "    fig, ax = plt.subplots(figsize=(16, 7))\n",
    "    \n",
    "    for city_name, city_series in city_data.items():\n",
    "        color = CITY_COLORS.get(city_name, \"#666\")\n",
    "        # Smooth with 7-day rolling mean for clearer comparison\n",
    "        smoothed = city_series.rolling(7).mean()\n",
    "        ax.plot(smoothed.index, smoothed.values, label=f\"{city_name} (7-day avg)\",\n",
    "                color=color, linewidth=2, alpha=0.85)\n",
    "    \n",
    "    ax.set_title(\"Geographical Temperature Patterns ‚Äî Major Cities (7-Day Rolling Average)\",\n",
    "                 fontsize=14, fontweight=\"bold\")\n",
    "    ax.set_xlabel(\"Date\")\n",
    "    ax.set_ylabel(\"Temperature (¬∞C)\")\n",
    "    ax.legend(loc=\"upper right\", fontsize=11)\n",
    "    ax.xaxis.set_major_formatter(mdates.DateFormatter(\"%Y-%m\"))\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(\"\\n  Temperature Summary by City:\")\n",
    "    print(f\"  {'City':<12s} {'Mean':>8s} {'Std':>8s} {'Min':>8s} {'Max':>8s}\")\n",
    "    print(f\"  {'‚îÄ'*48}\")\n",
    "    for city_name, city_series in city_data.items():\n",
    "        print(f\"  {city_name:<12s} {city_series.mean():>8.1f} {city_series.std():>8.1f} \"\n",
    "              f\"{city_series.min():>8.1f} {city_series.max():>8.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a25410f",
   "metadata": {},
   "source": [
    "---\n",
    "## 22. Deliverables Checklist and Reproducibility Notes\n",
    "\n",
    "### ‚úÖ Complete Assessment Checklist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d041e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 22. Deliverables Checklist ‚îÄ‚îÄ\n",
    "checklist = [\n",
    "    (\"PM Accelerator Mission\", \"Displayed\", \"README.md + Cell 2 of this notebook\"),\n",
    "    (\"Data Cleaning & Preprocessing\", \"‚úÖ Complete\", \"Cell 3 (cleaning pipeline)\"),\n",
    "    (\"EDA: Temp & Precip Visualizations\", \"‚úÖ Complete\", \"Cell 4 (distributions)\"),\n",
    "    (\"EDA: Trends & Correlations\", \"‚úÖ Complete\", \"Cells 5-6 (time series, heatmap)\"),\n",
    "    (\"Basic Forecasting Model + Metrics\", \"‚úÖ Complete\", \"Cells 9-10 (Naive, SARIMA)\"),\n",
    "    (\"Anomaly Detection (STL + IsoForest)\", \"‚úÖ Complete\", \"Cells 15-16\"),\n",
    "    (\"Multiple Forecasting Models Compared\", \"‚úÖ Complete\", \"Cells 9-14 (6 models)\"),\n",
    "    (\"Ensemble Model\", \"‚úÖ Complete\", \"Cell 13 (inv-MAE weighted)\"),\n",
    "    (\"Climate Analysis by Continent\", \"‚úÖ Complete\", \"Cell 17\"),\n",
    "    (\"Air Quality ‚Üî Weather Correlation\", \"‚úÖ Complete\", \"Cell 18\"),\n",
    "    (\"Feature Importance Analysis\", \"‚úÖ Complete\", \"Cell 19\"),\n",
    "    (\"Spatial/Geographic Temperature Map\", \"‚úÖ Complete\", \"Cell 20\"),\n",
    "    (\"Geographical Patterns\", \"‚úÖ Complete\", \"Cell 21\"),\n",
    "    (\"Well-organized README.md\", \"‚úÖ Complete\", \"README.md\"),\n",
    "    (\"Reproducible Pipeline\", \"‚úÖ Complete\", \"python main.py\"),\n",
    "]\n",
    "\n",
    "checklist_df = pd.DataFrame(checklist, columns=[\"Requirement\", \"Status\", \"Location\"])\n",
    "checklist_df.index = range(1, len(checklist_df) + 1)\n",
    "checklist_df.index.name = \"#\"\n",
    "\n",
    "display(HTML(\"<h3>üìã Assessment Deliverables Checklist</h3>\"))\n",
    "display(checklist_df.style.set_properties(**{\n",
    "    'text-align': 'left',\n",
    "}).set_table_styles([\n",
    "    {'selector': 'th', 'props': [('text-align', 'center'), ('background-color', '#1a1a2e'), ('color', 'white')]},\n",
    "]))\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  OUTPUT FILE LOCATIONS\")\n",
    "print(\"=\" * 65)\n",
    "outputs = {\n",
    "    \"Cleaned Data\": \"data/processed/weather_clean.parquet\",\n",
    "    \"Forecast Results\": \"reports/forecast_results.csv\",\n",
    "    \"Missing Values Chart\": \"reports/figures/missing_values.png\",\n",
    "    \"Temp/Precip Distributions\": \"reports/figures/temp_precip_distributions.png\",\n",
    "    \"City Time Series\": \"reports/figures/timeseries_major_cities.png\",\n",
    "    \"Correlation Heatmap\": \"reports/figures/correlation_heatmap.png\",\n",
    "    \"7-Day Model Comparison\": \"reports/figures/model_comparison_7day.png\",\n",
    "    \"14-Day Model Comparison\": \"reports/figures/model_comparison_14day.png\",\n",
    "    \"STL Anomaly Detection\": \"reports/figures/stl_anomaly_detection.png\",\n",
    "    \"Feature Importance\": \"reports/figures/feature_importance.png\",\n",
    "    \"Climate by Continent\": \"reports/figures/monthly_climate_continent.png\",\n",
    "    \"Air Quality Correlation\": \"reports/figures/air_quality_weather_corr.png\",\n",
    "    \"Spatial Temperature Map\": \"reports/figures/spatial_temperature_map.png\",\n",
    "}\n",
    "\n",
    "for name, path in outputs.items():\n",
    "    full_path = PROJECT_ROOT / path\n",
    "    exists = \"‚úÖ\" if full_path.exists() else \"‚ùå\"\n",
    "    print(f\"  {exists} {name:<30s} ‚Üí {path}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  REPRODUCTION COMMANDS\")\n",
    "print(\"=\" * 65)\n",
    "print(\"  # 1. Install dependencies\")\n",
    "print(\"  pip install -r requirements.txt\")\n",
    "print(\"\")\n",
    "print(\"  # 2. Run the full pipeline\")\n",
    "print(\"  python main.py\")\n",
    "print(\"\")\n",
    "print(\"  # 3. Run the demo walkthrough\")\n",
    "print(\"  python demo.py\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f715621c",
   "metadata": {},
   "source": [
    "---\n",
    "## 23. Export Report as PDF\n",
    "\n",
    "This section exports the complete notebook as a **PDF report** for presentation.\n",
    "\n",
    "### Option A: Automated Export (run the cell below)\n",
    "### Option B: Manual Export\n",
    "- In VS Code: **File ‚Üí Export As ‚Üí PDF**\n",
    "- In Jupyter: **File ‚Üí Download as ‚Üí PDF via LaTeX**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43b3e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ‚îÄ‚îÄ 23. Export Report as PDF ‚îÄ‚îÄ\n",
    "NOTEBOOK_NAME = \"weather_forecast_report\"\n",
    "NOTEBOOK_PATH = PROJECT_ROOT / \"notebooks\" / f\"{NOTEBOOK_NAME}.ipynb\"\n",
    "PDF_OUTPUT = REPORTS_DIR / f\"{NOTEBOOK_NAME}.pdf\"\n",
    "HTML_OUTPUT = REPORTS_DIR / f\"{NOTEBOOK_NAME}.html\"\n",
    "\n",
    "def export_to_pdf():\n",
    "    \"\"\"Export this notebook as a PDF using nbconvert.\"\"\"\n",
    "    print(\"=\" * 65)\n",
    "    print(\"  EXPORTING NOTEBOOK AS PDF\")\n",
    "    print(\"=\" * 65)\n",
    "    \n",
    "    # Try PDF export first (requires LaTeX)\n",
    "    try:\n",
    "        print(\"\\n  Attempting PDF export via nbconvert + LaTeX...\")\n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                sys.executable, \"-m\", \"jupyter\", \"nbconvert\",\n",
    "                \"--to\", \"pdf\",\n",
    "                \"--no-input\",\n",
    "                \"--output-dir\", str(REPORTS_DIR),\n",
    "                str(NOTEBOOK_PATH),\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300,\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"  ‚úÖ PDF exported successfully!\")\n",
    "            print(f\"     üìÑ {PDF_OUTPUT}\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  PDF export failed (LaTeX may not be installed).\")\n",
    "            print(f\"     stderr: {result.stderr[:200]}\")\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError) as e:\n",
    "        print(f\"  ‚ö†Ô∏è  PDF export error: {e}\")\n",
    "    \n",
    "    # Fallback: HTML export\n",
    "    try:\n",
    "        print(\"\\n  Falling back to HTML export...\")\n",
    "        result = subprocess.run(\n",
    "            [\n",
    "                sys.executable, \"-m\", \"jupyter\", \"nbconvert\",\n",
    "                \"--to\", \"html\",\n",
    "                \"--no-input\",\n",
    "                \"--output-dir\", str(REPORTS_DIR),\n",
    "                str(NOTEBOOK_PATH),\n",
    "            ],\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            timeout=300,\n",
    "        )\n",
    "        if result.returncode == 0:\n",
    "            print(f\"  ‚úÖ HTML exported successfully!\")\n",
    "            print(f\"     üìÑ {HTML_OUTPUT}\")\n",
    "            print(f\"     üí° Open in browser and Print ‚Üí Save as PDF\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"  ‚ö†Ô∏è  HTML export also failed: {result.stderr[:200]}\")\n",
    "    except (subprocess.TimeoutExpired, FileNotFoundError) as e:\n",
    "        print(f\"  ‚ö†Ô∏è  HTML export error: {e}\")\n",
    "    \n",
    "    return False\n",
    "\n",
    "# Attempt export\n",
    "success = export_to_pdf()\n",
    "\n",
    "if not success:\n",
    "    print(\"\\n\" + \"=\" * 65)\n",
    "    print(\"  MANUAL EXPORT INSTRUCTIONS\")\n",
    "    print(\"=\" * 65)\n",
    "    print(\"\"\"\n",
    "  If automated export failed, you can export manually:\n",
    "\n",
    "  üìå VS Code:\n",
    "     1. Open this notebook in VS Code\n",
    "     2. Click '...' (More Actions) in the notebook toolbar\n",
    "     3. Select 'Export As' ‚Üí 'PDF'\n",
    "\n",
    "  üìå Jupyter Lab:\n",
    "     1. File ‚Üí Export Notebook As ‚Üí PDF\n",
    "\n",
    "  üìå Command Line (requires LaTeX):\n",
    "     jupyter nbconvert --to pdf --no-input notebooks/weather_forecast_report.ipynb\n",
    "\n",
    "  üìå Command Line (HTML fallback):\n",
    "     jupyter nbconvert --to html --no-input notebooks/weather_forecast_report.ipynb\n",
    "     # Then open HTML in browser and Print ‚Üí Save as PDF\n",
    "    \"\"\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 65)\n",
    "print(\"  üéâ  REPORT GENERATION COMPLETE!\")\n",
    "print(\"=\" * 65)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db9a22f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# üéâ End of Report\n",
    "\n",
    "**PM Accelerator ‚Äì Weather Trend Forecasting Assessment**\n",
    "\n",
    "This report covers all Basic and Advanced requirements:\n",
    "- ‚úÖ Data Cleaning & Preprocessing\n",
    "- ‚úÖ Exploratory Data Analysis with Temperature & Precipitation Visualizations\n",
    "- ‚úÖ Forecasting Models (Naive, Seasonal Naive, SARIMA, Prophet, GBR, Ensemble)\n",
    "- ‚úÖ Model Evaluation (MAE, RMSE, sMAPE)\n",
    "- ‚úÖ Anomaly Detection (STL Decomposition + Isolation Forest)\n",
    "- ‚úÖ Climate Analysis by Continent\n",
    "- ‚úÖ Air Quality ‚Üî Weather Correlation\n",
    "- ‚úÖ Feature Importance Analysis\n",
    "- ‚úÖ Spatial Temperature Mapping\n",
    "- ‚úÖ Geographical Pattern Analysis\n",
    "\n",
    "---\n",
    "*Generated by the PM Accelerator Weather Trend Forecasting Pipeline*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
